# chatbot_logic.py
from googleapiclient.discovery import build
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import joblib
from nltk.tokenize import word_tokenize
import nltk
import requests
import random
from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

# T·∫£i t√†i nguy√™n punkt_tab (ch·ªâ c·∫ßn ch·∫°y m·ªôt l·∫ßn)
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

# API Keys (Thay b·∫±ng API key th·∫≠t c·ªßa b·∫°n)
GOOGLE_API_KEY = "AIzaSyCwN16UhmZsKYazVd2CQrjDE3kj6XrAJbY"
GOOGLE_CSE_ID = "519d867d7db1f49df"
OPENWEATHER_API_KEY = "2a39ac296197c2f8ad13922c8bf17686"
NEWSAPI_KEY = "e9dffaa1921e4f43ac0a738a5fcc76b9"

# Load m√¥ h√¨nh BlenderBot
model_name = "facebook/blenderbot-3B"
try:
    tokenizer = BlenderbotTokenizer.from_pretrained(model_name)
    model = BlenderbotForConditionalGeneration.from_pretrained(model_name)
except Exception as e:
    print(f"L·ªói khi t·∫£i m√¥ h√¨nh BlenderBot: {e}")
    tokenizer = None
    model = None

# H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
def preprocess_text(text):
    text = text.lower()
    tokens = word_tokenize(text)
    return ' '.join(tokens)

# H√†m l·∫•y t√≥m t·∫Øt t·ª´ Wikipedia
def get_wikipedia_summary(topic):
    headers = {
        "User-Agent": "MyChatBot (your_email@example.com)"
    }
    topic = topic.replace(" ", "_")
    url = f"https://vi.wikipedia.org/api/rest_v1/page/summary/{topic}"
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        if "extract" in data:
            return data["extract"]
        else:
            return None
    except requests.exceptions.RequestException:
        return None

# H√†m l·∫•y th√¥ng tin th·ªùi ti·∫øt t·ª´ OpenWeatherMap
def get_weather(city):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={OPENWEATHER_API_KEY}&units=metric&lang=vi"
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        if data.get("cod") == 200:
            weather = data["weather"][0]["description"]
            temp = data["main"]["temp"]
            return f"H√¥m nay ·ªü {city} tr·ªùi {weather}, nhi·ªát ƒë·ªô kho·∫£ng {temp}¬∞C. Hy v·ªçng b·∫°n c√≥ m·ªôt ng√†y tuy·ªát v·ªùi nh√©!"
        else:
            return f"Xin l·ªói, m√¨nh kh√¥ng t√¨m ƒë∆∞·ª£c th√¥ng tin ·ªü {city}. B·∫°n th·ª≠ v·ªõi ƒë·ªãa ƒëi·ªÉm kh√°c ƒë∆∞·ª£c kh√¥ng?"
    except requests.exceptions.RequestException:
        return "M√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c khi l·∫•y th√¥ng tin, b·∫°n th·ª≠ l·∫°i sau nh√©!"

# T√≠ch h·ª£p ph·∫£n h·ªìi t·ª± nhi√™n v√†o h√†m x·ª≠ l√Ω c√¢u h·ªèi
def tu_van_nganh(cau_hoi, danh_sach_nganh):
    cau_hoi_processed = preprocess_text(cau_hoi)

    # Ki·ªÉm tra c√¢u h·ªèi li√™n quan ƒë·∫øn th·ªùi ti·∫øt
    if "h√¥m nay" in cau_hoi_processed:
        words = cau_hoi_processed.split()
        city = None
        for word in words:
            if word not in ["h√¥m", "nay", "·ªü", "t·∫°i"]:
                city = word
                break
        if city:
            weather_info = get_weather(city)
            return "", "", weather_info
        else:
            return "", "", "B·∫°n mu·ªën bi·∫øt th·ªùi ti·∫øt ·ªü ƒë√¢u, h√£y cho m√¨nh bi·∫øt nh√©!"

    # H√†m t√πy ch·ªânh ph·∫£n h·ªìi
def custom_response(source, content):
    if source == "Wikipedia":
        return f"D∆∞·ªõi ƒë√¢y l√† th√¥ng tin m√¨nh t√¨m ƒë∆∞·ª£c t·ª´ Wikipedia: {content}"
    elif source == "NewsAPI":
        return f"D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë tin t·ª©c m·ªõi nh·∫•t m√¨nh t√¨m ƒë∆∞·ª£c: {content}"
    elif source == "OpenWeather":
        return f"D∆∞·ªõi ƒë√¢y l√† th√¥ng tin th·ªùi ti·∫øt hi·ªán t·∫°i: {content}"
    elif source == "Google CSE":
        return f"D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ t√¨m ki·∫øm t·ª´ Google: {content}"
    else:
        return "M√¨nh kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p, b·∫°n th·ª≠ l·∫°i nh√©!"

# T√≠ch h·ª£p v√†o h√†m x·ª≠ l√Ω c√¢u h·ªèi
def tu_van_nganh(cau_hoi, danh_sach_nganh):
    cau_hoi_processed = preprocess_text(cau_hoi)

    # Ki·ªÉm tra c√¢u h·ªèi li√™n quan ƒë·∫øn Wikipedia
    if "wiki" in cau_hoi_processed:
        wiki_summary = get_wikipedia_summary(cau_hoi)
        if wiki_summary:
            return "", "", custom_response("Wikipedia", wiki_summary)
        else:
            return "", "", "M√¨nh kh√¥ng t√¨m th·∫•y th√¥ng tin t·ª´ Wikipedia, b·∫°n th·ª≠ h·ªèi l·∫°i nh√©!"

    # Ki·ªÉm tra c√¢u h·ªèi li√™n quan ƒë·∫øn tin t·ª©c
    if "tin t·ª©c" in cau_hoi_processed:
        news_info = get_news(cau_hoi)
        if news_info:
            return "", "", custom_response("NewsAPI", news_info)
        else:
            return "", "", "M√¨nh kh√¥ng t√¨m th·∫•y tin t·ª©c li√™n quan, b·∫°n th·ª≠ cung c·∫•p t·ª´ kh√≥a kh√°c nh√©!"

    # Ki·ªÉm tra c√¢u h·ªèi li√™n quan ƒë·∫øn th·ªùi ti·∫øt
    if "h√¥m nay" in cau_hoi_processed:
        words = cau_hoi_processed.split()
        city = None
        for word in words:
            if word not in ["h√¥m", "nay", "·ªü", "t·∫°i"]:
                city = word
                break
        if city:
            weather_info = get_weather(city)
            return "", "", custom_response("OpenWeather", weather_info)
        else:
            return "", "", "B·∫°n mu·ªën bi·∫øt th·ªùi ti·∫øt ·ªü ƒë√¢u, h√£y cho m√¨nh bi·∫øt nh√©!"

    # Ki·ªÉm tra c√¢u h·ªèi li√™n quan ƒë·∫øn Google CSE
    if "google" in cau_hoi_processed:
        google_result = search_google(cau_hoi)
        if google_result:
            return "", "", custom_response("Google CSE", google_result)
        else:
            return "", "", "M√¨nh kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ t·ª´ Google, b·∫°n th·ª≠ l·∫°i nh√©!"

# H√†m l·∫•y tin t·ª©c t·ª´ NewsAPI
def get_news(keyword):
    url = f"https://newsapi.org/v2/everything?q={keyword}&apiKey={NEWSAPI_KEY}&language=vi&pageSize=3"
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        if data.get("status") == "ok" and data.get("articles"):
            news = "D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë tin t·ª©c li√™n quan:\n"
            for i, article in enumerate(data["articles"], 1):
                title = article["title"]
                url = article["url"]
                news += f"{i}. {title}\n   Link: {url}\n"
            return news
        else:
            return None
    except requests.exceptions.RequestException:
        return None

# H√†m t√¨m ki·∫øm th√¥ng tin t·ª´ Google Custom Search
def search_google(query):
    service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
    try:
        res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, num=3).execute()
        if "items" in res and len(res["items"]) > 0:
            thong_tin = "D∆∞·ªõi ƒë√¢y l√† th√¥ng tin t√¨m ki·∫øm t·ª´ Google:\n"
            for i, item in enumerate(res["items"], 1):
                title = item.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
                snippet = item.get("snippet", "Kh√¥ng c√≥ m√¥ t·∫£")
                link = item.get("link", "Kh√¥ng c√≥ li√™n k·∫øt")
                thong_tin += f"{i}. {title}\n   - M√¥ t·∫£: {snippet}\n   - Link: {link}\n"
            return thong_tin
        else:
            return None
    except Exception:
        return None

# H√†m tr√≤ chuy·ªán v·ªõi BlenderBot
def chat_with_blenderbot(message):
    if tokenizer is None or model is None:
        return "M√¨nh kh√¥ng th·ªÉ tr√≤ chuy·ªán v√¨ m√¥ h√¨nh BlenderBot ch∆∞a ƒë∆∞·ª£c t·∫£i. H√£y ki·ªÉm tra l·∫°i!"
    try:
        inputs = tokenizer([message], return_tensors="pt")
        reply_ids = model.generate(**inputs)
        reply = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]
        return reply
    except Exception as e:
        return f"L·ªói khi tr√≤ chuy·ªán v·ªõi BlenderBot: {str(e)}"

# Load m√¥ h√¨nh TF-IDF ƒë√£ l∆∞u
try:
    vectorizer = joblib.load("tfidf_vectorizer.joblib")
    tfidf_matrix = joblib.load("tfidf_matrix.joblib")
    ky_nangs = joblib.load("ky_nangs.joblib")
except Exception as e:
    print(f"L·ªói khi load m√¥ h√¨nh TF-IDF: {e}. Vui l√≤ng ch·∫°y file train_tfidf.py tr∆∞·ªõc!")
    vectorizer = None
    tfidf_matrix = None
    ky_nangs = []

# Danh s√°ch c√°c c√¢u tr·∫£ l·ªùi ch√†o h·ªèi
greeting_responses = [
    "Ch√†o b·∫°n! M√¨nh l√† chatbot h·ªó tr·ª£ ng√†nh h·ªçc, h√¥m nay m√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?",
    "Xin ch√†o! M√¨nh ·ªü ƒë√¢y ƒë·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ c√°c ng√†nh h·ªçc ho·∫∑c b·∫•t k·ª≥ th√¥ng tin n√†o b·∫°n c·∫ßn. B·∫°n mu·ªën h·ªèi g√¨ n√†o?",
    "Ch√†o b·∫°n! R·∫•t vui ƒë∆∞·ª£c tr√≤ chuy·ªán v·ªõi b·∫°n. H√¥m nay b·∫°n mu·ªën t√¨m hi·ªÉu v·ªÅ ng√†nh h·ªçc hay ch·ªâ mu·ªën tr√≤ chuy·ªán th√¥i? üòä",
    "Hello! M√¨nh l√† chatbot si√™u th√¢n thi·ªán ƒë√¢y, b·∫°n kh·ªèe kh√¥ng? H√¥m nay m√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?",
    "Ch√†o b·∫°n! H√¥m nay b·∫°n th·∫ø n√†o? M√¨nh s·∫µn s√†ng gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ ng√†nh h·ªçc ho·∫∑c tr√≤ chuy·ªán vui v·∫ª! üòÑ"
]

casual_responses = [
    "·ªí, c√¢u h·ªèi n√†y th√∫ v·ªã ƒë·∫•y! Nh∆∞ng m√¨nh ch∆∞a bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, b·∫°n c√≥ th·ªÉ n√≥i th√™m chi ti·∫øt kh√¥ng?",
    "Hmmm, m√¨nh kh√¥ng ch·∫Øc l·∫Øm, nh∆∞ng m√¨nh c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu th√™m n·∫øu b·∫°n mu·ªën!",
    "C√¢u h·ªèi n√†y h∆°i ngo√†i chuy√™n m√¥n c·ªßa m√¨nh, nh∆∞ng m√¨nh nghƒ© ch√∫ng ta c√≥ th·ªÉ c√πng t√¨m hi·ªÉu nh√©!",
    "M√¨nh kh√¥ng c√≥ th√¥ng tin ch√≠nh x√°c, nh∆∞ng m√¨nh c√≥ th·ªÉ k·ªÉ b·∫°n nghe m·ªôt c√¢u chuy·ªán vui n·∫øu b·∫°n mu·ªën! üòÑ",
    "Ch√†, m√¨nh ch∆∞a ƒë∆∞·ª£c train ƒë·ªÉ tr·∫£ l·ªùi c√¢u n√†y, nh∆∞ng m√¨nh c√≥ th·ªÉ t√¨m ki·∫øm gi√∫p b·∫°n! B·∫°n mu·ªën m√¨nh t√¨m g√¨?",
    "M√¨nh l√† chatbot n√™n kh√¥ng c√≥ √Ω ki·∫øn c√° nh√¢n, nh∆∞ng n·∫øu m√¨nh l√† con ng∆∞·ªùi, m√¨nh s·∫Ω r·∫•t t√≤ m√≤ v·ªÅ c√¢u h·ªèi n√†y! B·∫°n nghƒ© sao?",
    "C√¢u h·ªèi n√†y l√†m m√¨nh nh·ªõ ƒë·∫øn m·ªôt l·∫ßn m√¨nh t√¨m hi·ªÉu v·ªÅ m·ªôt ch·ªß ƒë·ªÅ ho√†n to√†n kh√°c... √Ä m√† th√¥i, b·∫°n mu·ªën m√¨nh t√¨m th√¥ng tin g√¨ th√™m kh√¥ng? üòÑ",
    "H√¥m nay b·∫°n th·∫ø n√†o? M√¨nh th√¨ ƒëang r·∫•t vui v√¨ ƒë∆∞·ª£c tr√≤ chuy·ªán v·ªõi b·∫°n! üòä",
    "M√¨nh kh√¥ng ch·∫Øc l·∫Øm, nh∆∞ng n·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ k·ªÉ cho b·∫°n m·ªôt v√†i ƒëi·ªÅu th√∫ v·ªã m√† m√¨nh bi·∫øt! B·∫°n mu·ªën nghe v·ªÅ ch·ªß ƒë·ªÅ g√¨?"
]
# H√†m ki·ªÉm tra c√¢u ch√†o h·ªèi
def is_greeting(cau_hoi_processed):
    greetings = ["ch√†o", "xin ch√†o", "hi", "hello", "hallo", "chao", "t·ªï", "t√¥"]
    return any(greeting in cau_hoi_processed for greeting in greetings)

# H√†m ki·ªÉm tra c√¢u h·ªèi mang t√≠nh tr√≤ chuy·ªán
def is_conversational(cau_hoi_processed):
    conversational_phrases = [
        "h√¥m nay th·∫ø n√†o", "c√≥ th·ªÉ gi√∫p g√¨", "b·∫°n kh·ªèe kh√¥ng", "t√¨nh h√¨nh th·∫ø n√†o", 
        "h√¥m nay ra sao", "b·∫°n ƒëang l√†m g√¨", "c√≥ g√¨ vui kh√¥ng", "h√¥m nay b·∫°n th·∫ø n√†o",
        "nghi th·ª©c", "t·ªï", "khang", "b·∫°n c√≥ kh·ªèe kh√¥ng", "h√¥m nay c√≥ g√¨ m·ªõi"
    ]
    return any(phrase in cau_hoi_processed for phrase in conversational_phrases)

def tu_van_nganh(cau_hoi, danh_sach_nganh):
    cau_hoi_processed = preprocess_text(cau_hoi)

    # Ki·ªÉm tra c√¢u ch√†o h·ªèi
    if is_greeting(cau_hoi_processed):
        return "Ch√†o h·ªèi", "", random.choice(greeting_responses)

    # Ki·ªÉm tra c√¢u h·ªèi mang t√≠nh tr√≤ chuy·ªán
    if is_conversational(cau_hoi_processed):
        blender_response = chat_with_blenderbot(cau_hoi)
        return "Tr√≤ chuy·ªán", "", f"M√¨nh ƒëang r·∫•t ·ªïn, c·∫£m ∆°n b·∫°n ƒë√£ h·ªèi! üòä {blender_response}"

    # Ki·ªÉm tra n·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn th·ªùi ti·∫øt
    if "th·ªùi ti·∫øt" in cau_hoi_processed:
        words = cau_hoi_processed.split()
        city = None
        for word in words:
            if word not in ["th·ªùi", "ti·∫øt", "·ªü", "t·∫°i"]:
                city = word
                break
        if city:
            weather_info = get_weather(city)
            if weather_info:
                return "Th√¥ng tin th·ªùi ti·∫øt", "", weather_info
            else:
                return "Kh√¥ng r√µ", "", "Kh√¥ng t√¨m th·∫•y th√¥ng tin th·ªùi ti·∫øt cho th√†nh ph·ªë n√†y."
        else:
            return "Kh√¥ng r√µ", "", "Vui l√≤ng cung c·∫•p t√™n th√†nh ph·ªë ƒë·ªÉ tra c·ª©u th·ªùi ti·∫øt."

    # Ki·ªÉm tra n·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn tin t·ª©c
    if "tin t·ª©c" in cau_hoi_processed or "m·ªõi nh·∫•t" in cau_hoi_processed:
        words = cau_hoi_processed.split()
        keyword = None
        for word in words:
            if word not in ["tin", "t·ª©c", "m·ªõi", "nh·∫•t", "v·ªÅ"]:
                keyword = word
                break
        if keyword:
            news_info = get_news(keyword)
            if news_info:
                return "Tin t·ª©c", "", news_info
            else:
                return "Kh√¥ng r√µ", "", "Kh√¥ng t√¨m th·∫•y tin t·ª©c li√™n quan."
        else:
            return "Kh√¥ng r√µ", "", "Vui l√≤ng cung c·∫•p t·ª´ kh√≥a ƒë·ªÉ t√¨m tin t·ª©c."

    # Ki·ªÉm tra t·ª´ kh√≥a tr∆∞·ªõc (n·∫øu kh·ªõp ch√≠nh x√°c th√¨ ∆∞u ti√™n)
    for ky_nang, info in danh_sach_nganh.items():
        if ky_nang in cau_hoi_processed or f"gi·ªèi {ky_nang}" in cau_hoi_processed or f"th√≠ch {ky_nang}" in cau_hoi_processed:
            return info["ng√†nh"], info["tr∆∞·ªùng"], info["m√¥_ta"]

    # N·∫øu kh√¥ng kh·ªõp t·ª´ kh√≥a, d√πng TF-IDF ƒë·ªÉ t√¨m ng√†nh ph√π h·ª£p
    if vectorizer is not None and tfidf_matrix is not None:
        cau_hoi_tfidf = vectorizer.transform([cau_hoi_processed])
        cosine_similarities = cosine_similarity(cau_hoi_tfidf, tfidf_matrix)
        best_match_idx = np.argmax(cosine_similarities)
        best_score = cosine_similarities[0][best_match_idx]

        if best_score > 0.1:
            ky_nang = ky_nangs[best_match_idx]
            info = danh_sach_nganh[ky_nang]
            return info["ng√†nh"], info["tr∆∞·ªùng"], info["m√¥_ta"]

    # N·∫øu kh√¥ng t√¨m th·∫•y trong d·ªØ li·ªáu ng√†nh h·ªçc, th·ª≠ t√¨m tr√™n Wikipedia
    wiki_summary = get_wikipedia_summary(cau_hoi)
    if wiki_summary:
        return "Kh√¥ng r√µ", "Ch∆∞a x√°c ƒë·ªãnh", wiki_summary

    # N·∫øu kh√¥ng t√¨m th·∫•y tr√™n Wikipedia, th·ª≠ t√¨m tr√™n Google
    google_result = search_google(cau_hoi)
    if google_result:
        return "Kh√¥ng r√µ", "Ch∆∞a x√°c ƒë·ªãnh", google_result

    # N·∫øu kh√¥ng t√¨m th·∫•y g√¨, d√πng BlenderBot ƒë·ªÉ tr·∫£ l·ªùi
    blender_response = chat_with_blenderbot(cau_hoi)
    return "Kh√¥ng r√µ", "Ch∆∞a x√°c ƒë·ªãnh", blender_response

def lay_thong_tin_truong(truong):
    service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
    query = f"Th√¥ng tin v·ªÅ {truong} ng√†nh h·ªçc Vi·ªát Nam"
    try:
        res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, num=3).execute()
        if "items" in res and len(res["items"]) > 0:
            thong_tin = "D∆∞·ªõi ƒë√¢y l√† th√¥ng tin chi ti·∫øt t·ª´ Google:\n"
            for i, item in enumerate(res["items"], 1):
                title = item.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
                snippet = item.get("snippet", "Kh√¥ng c√≥ m√¥ t·∫£")
                link = item.get("link", "Kh√¥ng c√≥ li√™n k·∫øt")
                thong_tin += f"{i}. {title}\n   - M√¥ t·∫£: {snippet}\n   - Link: {link}\n"
            return thong_tin
        else:
            return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ {truong} tr√™n web."
    except Exception as e:
        return f"L·ªói khi t√¨m th√¥ng tin: {str(e)}. Vui l√≤ng th·ª≠ l·∫°i sau."